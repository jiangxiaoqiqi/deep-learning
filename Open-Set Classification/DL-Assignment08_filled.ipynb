{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 8: Open-Set Classification\n",
    "=====================================\n",
    "\n",
    "\n",
    "Microsoft Forms Document: https://forms.office.com/r/xY9sQDQdGh\n",
    "\n",
    "We select the MNIST dataset and define several classes to be known, known unknown (used as negative class during training) and unknown unknown (not used for training at all).\n",
    "\n",
    "Task 1: Target Vectors\n",
    "----------------------\n",
    "\n",
    "For our training dataset, we want to use four classes of MNIST digits (4,5,8,9) as known classes and four (0,2,3,7) as known unknowns. \n",
    "The remaining two classes shall be ignored during training and validation, amd only be used for testing purposes.\n",
    "\n",
    "When we want to train with our adapted softmax function, we need to assign the correct target vectors for the classes. \n",
    "These are $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$ and $(0,0,0,1)$ for the known classes, respectively. \n",
    "For known unknown classes, the target vector is $\\left(\\frac14,\\frac14,\\frac14,\\frac14\\right)$, throughout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# define the three types of classes\n",
    "known_classes = (4, 5, 8, 9)\n",
    "known_unknown_classes = (0, 2, 3, 7)\n",
    "unknown_classes = (1, 6)\n",
    "\n",
    "O = len(known_classes)\n",
    "# define one-hot vectors\n",
    "eye = torch.eye(O)\n",
    "same = torch.tensor([1/O]*O)\n",
    "\n",
    "def target_vector(index):\n",
    "    # select correct one-hot vector for known classes, and the 1/O-vectors for unknown classes\n",
    "    return eye[known_classes.index(index)] if index in known_classes else same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Check your Target Vectors\n",
    "---------------------------------\n",
    "\n",
    "Test that your target vectors are correct, for all tpyes of known and unknown samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 tensor([1., 0., 0., 0.])\n",
      "5 tensor([0., 1., 0., 0.])\n",
      "8 tensor([0., 0., 1., 0.])\n",
      "9 tensor([0., 0., 0., 1.])\n",
      "0 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "2 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "3 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "7 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "1 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "6 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n"
     ]
    }
   ],
   "source": [
    "# check that the target vectors for known classes are correct\n",
    "for index in known_classes:\n",
    "    t = target_vector(index)\n",
    "    print(index,t)\n",
    "    assert max(t) == 1\n",
    "    assert sum(t) == 1\n",
    "\n",
    "# check that the target vectors for unknown classes are correct\n",
    "for index in known_unknown_classes + unknown_classes:\n",
    "    t = target_vector(index)\n",
    "    print(index,t)\n",
    "    assert max(t) == 0.25\n",
    "    assert sum(t) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 and 3: Training Dataset\n",
    "------------------------\n",
    "We rely on the MNIST dataset implementation from PyTorch and adapt some parts of it. \n",
    "Mainly, we will let PyTorch load the dataset by calling the base class constructor and modify the `self.data` and `self.targets` ourselves.\n",
    "Additionally, we need to implement the index function to return the data and targets in the desired format.\n",
    "\n",
    "Since Jupyter Notebook does not allow splitting classes over several code boxes, the two tasks are required to be solved in the same code box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(torchvision.datasets.MNIST):\n",
    "    def __init__(self, purpose=\"train\"):\n",
    "        # call base class constructor to handle the data loading\n",
    "        super(DataSet, self).__init__(\n",
    "          root=\"./temp\",\n",
    "          train = purpose == \"train\",\n",
    "          download = True\n",
    "        )\n",
    "\n",
    "        # select the valid classes based on the current purpose\n",
    "        valid_classes = known_classes + (unknown_classes if purpose==\"test\" else known_unknown_classes)\n",
    "        # select the samples that belongs to these classes\n",
    "        valid_samples = sum(self.targets == v for v in valid_classes).bool()\n",
    "        # sub-select the data of valid classes\n",
    "        self.data = self.data[valid_samples]\n",
    "        # select the targets of valid classes\n",
    "        self.targets = [target_vector(t) for t in self.targets[valid_samples]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # perform appropriate actions on the data and the targets\n",
    "        input = self.data[index][None].float()/255\n",
    "        target = self.targets[index]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2: Data Sets\n",
    "-----------------\n",
    "\n",
    "Instantiate the training dataset.\n",
    "Implement a data loader for the training dataset with a batch size of 64.\n",
    "Assure that all inputs are of the desired type and shape.\n",
    "Assert that the target values are in the correct format, and the sum of the target values per sample is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./temp/DataSet/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006a2230a32b41a0a9c3fd1dfba176b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/DataSet/raw/train-images-idx3-ubyte.gz to ./temp/DataSet/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./temp/DataSet/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c088509c49ba4d6e8d71c4d87c72fcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/DataSet/raw/train-labels-idx1-ubyte.gz to ./temp/DataSet/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./temp/DataSet/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281a833561684741ad5571f440d00361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/DataSet/raw/t10k-images-idx3-ubyte.gz to ./temp/DataSet/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./temp/DataSet/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6778f54736c24adb87719b3655f2526a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/DataSet/raw/t10k-labels-idx1-ubyte.gz to ./temp/DataSet/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate the training dataset\n",
    "train_set = DataSet(purpose=\"train\")\n",
    "train_loader = torch.utils.data.DataLoader(train_set, 64, shuffle=True)\n",
    "\n",
    "# assert that we have not filtered out all samples\n",
    "assert len(train_loader)\n",
    "\n",
    "# check the batch and assert valid data and sizes\n",
    "for x, t in train_loader:\n",
    "    assert len(x) <= 64\n",
    "    assert len(t) == len(x)\n",
    "    assert torch.all(torch.sum(t, axis=1) == 1)\n",
    "    assert x.shape == torch.Size([x.shape[0], 1, 28, 28])\n",
    "    assert x.dtype == torch.float32\n",
    "    assert torch.max(x) <= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Utility Function\n",
    "------------------------\n",
    "\n",
    "Implement a function that splits a batch of samples into known and unknown parts. For the known parts, also provide the target vectors.\n",
    "How can we know which of the data samples are known smaples, and which are unknown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_known_unknown(batch, targets):\n",
    "    # select the indexes at which known and unknown samples exist\n",
    "    known = torch.max(targets, dim=1)[0] == 1\n",
    "    unknown = ~known\n",
    "    # return the known samples, the targets of the known samples, as well as the unknown samples\n",
    "    return batch[known], targets[known], batch[unknown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Loss Function Implementation\n",
    "------------------------------------\n",
    "\n",
    "We implement a loss function that implements an autograd function, i.e., we define both the forward and the backward pass for our loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedSoftMax(torch.autograd.Function):\n",
    "\n",
    "    # implement the forward propagation\n",
    "    @staticmethod\n",
    "    def forward(ctx, logits, targets):\n",
    "        # compute the log probabilities via log_softmax\n",
    "        log_y = torch.nn.functional.log_softmax(logits,dim=1)\n",
    "        # save required values for backward pass\n",
    "        ctx.save_for_backward(log_y, targets)\n",
    "        # compute loss\n",
    "        loss = - torch.sum(log_y * targets)\n",
    "        return loss\n",
    "\n",
    "    # implement Jacobian\n",
    "    @staticmethod\n",
    "    def backward(ctx, result):\n",
    "        # get results stored from forward pass\n",
    "        log_y, targets = ctx.saved_tensors\n",
    "        # compute derivative of loss w.r.t. the logits\n",
    "        dJ_dy = torch.exp(log_y) - targets\n",
    "        # return the derivatives; none for derivative for the targets\n",
    "        return dJ_dy, None\n",
    "\n",
    "\n",
    "# DO NOT REMOVE!\n",
    "# here we set the adapted softmax function to be used later\n",
    "adapted_softmax = AdaptedSoftMax.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5a: Alternative Loss Function\n",
    "----------------------------------\n",
    "\n",
    "If the implementation of the autograd function in Task 5 is too complicated, we can also rely on PyTorch to compute the gradient for us.\n",
    "In this case, we only need to define the forward pass, i.e., the loss function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapted_softmax_alt(logits, targets):\n",
    "    # compute cross-entropy loss on top of softmax values of the logits\n",
    "    loss = - torch.mean(logits * targets) + torch.mean(torch.logsumexp(logits, dim=1)) / targets.shape[1]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Confidence Evaluation\n",
    "-----------------------------\n",
    "\n",
    "Implement a function to compute the confidence value for a given batch of samples. Make sure to split the batch between known and unknown samples, and compute the confidence value for both separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(logits, targets):\n",
    "    # comupte softmax confidences\n",
    "    confidences = torch.nn.functional.softmax(logits, dim=1)\n",
    "    # split between known and unknown\n",
    "    known_confidences, known_targets, unknown_confidences = split_known_unknown(confidences, targets)\n",
    "    # compute confidence score for known targets\n",
    "    conf_known = sum(known_confidences[known_targets.bool()])\n",
    "    # compute confidence score for unknown targets\n",
    "    conf_unknown = torch.sum(1 - torch.max(unknown_confidences, dim=1)[0] + 1/O)\n",
    "    return conf_known + conf_unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3: Check Confidence Implementation\n",
    "---------------------------------------\n",
    "\n",
    "Test that your confidence implementation does what it is supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select good logit vectors for known and unknown classes\n",
    "logits = torch.tensor([[10.,0.,0.,0.,],[-10.,0.,-10.,-10.],[0.,0.,0.,0.]])\n",
    "# select the according target vectors for these classes\n",
    "targets = torch.stack([target_vector(known_classes[0]), target_vector(known_classes[1]), target_vector(known_unknown_classes[0])])\n",
    "\n",
    "# the confidence should be close to 1 for all cases\n",
    "assert 3 - confidence(logits,targets) < 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Network Definition\n",
    "--------------------------\n",
    "\n",
    "We define our own small-scale network to classify known and unknown samples for MNIST.\n",
    "We basically use the same convolutional network as in Assignment 6, with some small adaptations.\n",
    "However, this time we need to implement our own network model since we need to modify our network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, Q1, Q2, K, O):\n",
    "        # call base class constrcutor\n",
    "        super(Network, self).__init__()\n",
    "        # define convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=Q1, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=Q1, out_channels=Q2, kernel_size=5, stride=1, padding=2)\n",
    "        # pooling and activation functions will be re-used for the different stages\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        # define fully-connected layers\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(7*7*Q2, K, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(K, O, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute first layer of convolution, pooling and activation\n",
    "        a = self.act(self.pool(self.conv1(x)))\n",
    "        # compute second layer of convolution, pooling and activation\n",
    "        a = self.act(self.pool(self.conv2(x)))\n",
    "        # get the deep features as the output of the first fully-connected layer\n",
    "        deep_features = self.fc1(self.flatten(a))\n",
    "        # get the logits as the output of the second fully-connected layer\n",
    "        logits = self.fc2(deep_features)\n",
    "        # return both the logits and the deep features\n",
    "        return logits, deep_features\n",
    "\n",
    "\n",
    "# run on cuda device\n",
    "device = torch.device(\"cuda\")\n",
    "# create network with 20 hidden neurons in FC layer\n",
    "network = Network(32,32,20,4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8: Training Loop\n",
    "---------------------\n",
    "\n",
    "Instantiate everything that you need.\n",
    "Implement the training loop for 100 epochs.\n",
    "Compute the running training confidence and validation confidence and print them at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer with appropriate learning rate\n",
    "optimizer = torch.optim.SGD(...)\n",
    "\n",
    "# validation set and data loader\n",
    "validation_set = DataSet(\"valid\")\n",
    "validation_loader = ...\n",
    "\n",
    "for epoch in range(10):  # or 100\n",
    "    # evaluate average confidence for training and validation set\n",
    "    train_conf = validation_conf = 0.0\n",
    "\n",
    "    for x, t in train_loader:\n",
    "        # extract logits (and deep features) from network\n",
    "        ...\n",
    "        # compute our loss\n",
    "        ...\n",
    "\n",
    "        # perform weight update\n",
    "        ...\n",
    "\n",
    "        # compute training confidence\n",
    "        train_conf += ...\n",
    "\n",
    "    # compute validation comfidence\n",
    "    with torch.no_grad():\n",
    "        for x, t in validation_loader:\n",
    "            # extract logits (and deep features)\n",
    "            ...\n",
    "            # compute validation confidence\n",
    "            validation_conf += ...\n",
    "\n",
    "    # print average confidence for training and validation\n",
    "    print(\n",
    "        f\"\\rEpoch {epoch}; train: {train_conf/len(train_set):1.5f}, val: {validation_conf/len(validation_set):1.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 9: Feature Magnitude Plot\n",
    "------------------------------\n",
    "\n",
    "Take the validation and test sets and plot their feature magnitude as histogram, based on the pre-trained network and split between known, known unknown (validation set) and unknown unknown (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate test set and according data loader\n",
    "test_set = DataSet(\"test\")\n",
    "test_loader = ...\n",
    "\n",
    "# collect feature magnitudes for\n",
    "known, known_unknown, unknown = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # extract deep features magnitudes for validation set\n",
    "    for x, t in validation_loader:\n",
    "        # extract deep features (and logits)\n",
    "        ...\n",
    "        # compute norms\n",
    "        ...\n",
    "        # split between known and unknown\n",
    "        ...\n",
    "        # collect norms of known samples\n",
    "        known.extend(...)\n",
    "        # collect norms of known unknwown samples\n",
    "        known_unknown.extend(...)\n",
    "\n",
    "    for x, t in test_loader:\n",
    "        # extract deep features (and logits)\n",
    "        _, f = network(x.to(device))\n",
    "        # compute norms\n",
    "        ...\n",
    "        # split between known and unknown\n",
    "        ...\n",
    "        # collect norms of known samples\n",
    "        ...\n",
    "        # collect norms of unknown unknown samples\n",
    "        unknown.extend(...)\n",
    "\n",
    "\n",
    "# plot the norms as histograms\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure(figsize=(4, 2))\n",
    "\n",
    "# keep the same maximum magnitude; I could also compute it, but I am too lazy.\n",
    "max_mag = 20\n",
    "# plot the three histograms\n",
    "pyplot.hist(\n",
    "    known,\n",
    "    bins=100,\n",
    "    range=(0, max_mag),\n",
    "    density=True,\n",
    "    color=\"g\",\n",
    "    histtype=\"step\",\n",
    "    label=\"Known\",\n",
    ")\n",
    "pyplot.hist(\n",
    "    known_unknown,\n",
    "    bins=100,\n",
    "    range=(0, max_mag),\n",
    "    density=True,\n",
    "    color=\"b\",\n",
    "    histtype=\"step\",\n",
    "    label=\"Known Unknown\",\n",
    ")\n",
    "pyplot.hist(\n",
    "    unknown,\n",
    "    bins=100,\n",
    "    range=(0, max_mag),\n",
    "    density=True,\n",
    "    color=\"r\",\n",
    "    histtype=\"step\",\n",
    "    label=\"Unknown Unknown\",\n",
    ")\n",
    "\n",
    "# beautify plot\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"Deep Feature Magnitude\")\n",
    "pyplot.ylabel(\"Density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 10: Classification Evaluation\n",
    "----------------------------------\n",
    "\n",
    "For a fixed threshold of $\\tau=0.98$, compute CCR and FPR for the test set.\n",
    "A well-trained network can achieve a CCR of > 90% for an FPR < 10%.\n",
    "You might need to vary the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.98\n",
    "\n",
    "# count the correctly classified and the total number of known samples\n",
    "correct = known = 0\n",
    "# count the incorrectly classified and the total number of unknown samples\n",
    "false = unknown = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, t in test_loader:\n",
    "        # extract logits (and deep features)\n",
    "        ...\n",
    "        # compute softmax confidences\n",
    "        ...\n",
    "        # split between known and unknown\n",
    "        ...\n",
    "\n",
    "        # compute number of correctly classified knowns above threshold\n",
    "        correct += ...\n",
    "        known += ...\n",
    "\n",
    "        # compute number of incorrectly accepted known samples\n",
    "        false += ...\n",
    "        unknown += ...\n",
    "\n",
    "# print both rates\n",
    "print(f\"CCR: {correct} of {known} = {correct/known*100:2.2f}%\")\n",
    "print(f\"FPR: {false} of {unknown} = {false/unknown*100:2.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
